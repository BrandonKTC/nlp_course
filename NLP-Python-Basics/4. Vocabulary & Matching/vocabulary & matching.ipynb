{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9cf1d88",
   "metadata": {},
   "source": [
    "## Rule-based Matching\n",
    "spaCy offers a rule-matching tool called `Matcher` that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b732b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform standard imports\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d10d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb03b31",
   "metadata": {},
   "source": [
    "<font color=green>Here `matcher` is an object that pairs to the current `Vocab` object. We can add and remove specific named matchers to `matcher` as needed.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52483650",
   "metadata": {},
   "source": [
    "### Creating patterns\n",
    "In literature, the phrase 'solar power' might appear as one word or two, with or without a hyphen. In this section we'll develop a matcher named 'SolarPower' that finds all three:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5c001a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SolarPower\n",
    "pattern1 = [{'LOWER':'solarpower'}]\n",
    "# Solar-power\n",
    "pattern2 = [{'LOWER':'solar'},{'IS_PUNCT':True},{'LOWER':'power'}]\n",
    "# Solar power\n",
    "pattern3 = [{'LOWER':'solar'},{'LOWER':'power'}]\n",
    "\n",
    "matcher.add(key='SolarPower', patterns=[pattern1, pattern2, pattern3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d4938",
   "metadata": {},
   "source": [
    "Let's break this down:\n",
    "* `pattern1` looks for a single token whose lowercase text reads 'solarpower'\n",
    "* `pattern2` looks for two adjacent tokens that read 'solar' and 'power' in that order\n",
    "* `pattern3` looks for three adjacent tokens, with a middle token that can be any punctuation.<font color=green>*</font>\n",
    "\n",
    "<font color=green>\\* Remember that single spaces are not tokenized, so they don't count as punctuation.</font>\n",
    "<br>Once we define our patterns, we pass them into `matcher` with the name 'SolarPower', and set *callbacks* to `None` (more on callbacks later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f5049d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'The Solar Power industry continues to grow a solarpower increases. Solar-power is amazing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008ddb9b",
   "metadata": {},
   "source": [
    "### Applying the matcher to a Doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02e316c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 8, 9), (8656102463236116519, 11, 14)]\n"
     ]
    }
   ],
   "source": [
    "found_matches = matcher(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4436d5d6",
   "metadata": {},
   "source": [
    "`matcher` returns a list of tuples. Each tuple contains an ID for the match, with start & end tokens that map to the span `doc[start:end]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58910ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 1 3 Solar Power\n",
      "8656102463236116519 SolarPower 8 9 solarpower\n",
      "8656102463236116519 SolarPower 11 14 Solar-power\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = doc[start:end]                    # get the matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e0ad0",
   "metadata": {},
   "source": [
    "The `match_id` is simply the hash value of the `string_ID` 'SolarPower'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2fcc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove a particular patterns\n",
    "matcher.remove('SolarPower')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2eb297",
   "metadata": {},
   "source": [
    "### Setting pattern options and quantifiers\n",
    "You can make token rules optional by passing an `'OP':'*'` argument. This lets us streamline our patterns list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "610c8191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the patterns:\n",
    "pattern1 = [{'LOWER':'solarpower'}]\n",
    "# (solar.power or solar,power or *)\n",
    "pattern2 = [{'LOWER':'solar'},{'IS_PUNCT':True,'OP':'*'},{'LOWER':'power'}]\n",
    "\n",
    "# Add the new set of patterns to the 'SolarPower' matcher:\n",
    "matcher.add(key='SolarPower', patterns=[pattern1, pattern2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd7f45e",
   "metadata": {},
   "source": [
    "### Be careful with lemmas!\n",
    "If we wanted to match on both 'solar power' and 'solar powered', it might be tempting to look for the *lemma* of 'powered' and expect it to be 'power'. This is not always the case! The lemma of the *adjective* 'powered' is still 'powered':"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db447c",
   "metadata": {},
   "source": [
    "This found both two-word patterns, with and without the hyphen!\n",
    "\n",
    "The following quantifiers can be passed to the `'OP'` key:\n",
    "<table><tr><th>OP</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
    "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
    "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
    "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4424928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 0, 3), (8656102463236116519, 4, 7)]\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u'Solar--power is solar,power yay!')\n",
    "\n",
    "found_matches = matcher(doc2)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0c7d7",
   "metadata": {},
   "source": [
    "## Other token attributes\n",
    "Besides lemmas, there are a variety of token attributes we can use to determine matching rules:\n",
    "<table><tr><th>Attribute</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\n",
    "<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\n",
    "<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\n",
    "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\n",
    "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\n",
    "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\n",
    "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\n",
    "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\n",
    "<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2404a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 0 3 Solar--power\n",
      "8656102463236116519 SolarPower 4 7 solar,power\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = doc2[start:end]                    # get the matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "527ed98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeca82f",
   "metadata": {},
   "source": [
    "___\n",
    "## PhraseMatcher\n",
    "In the above section we used token patterns to perform rule-based matching. An alternative - and often more efficient - method is to match on terminology lists. In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into `matcher` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9388520",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13d7565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./reaganomics.txt', encoding='utf-8', errors='ignore') as f:\n",
    "    doc3 = nlp(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2024d3",
   "metadata": {},
   "source": [
    "For this exercise we're going to import a Wikipedia article on *Reaganomics*<br>\n",
    "Source: https://en.wikipedia.org/wiki/Reaganomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46fb063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics','free-market economics']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37832553",
   "metadata": {},
   "source": [
    "<font color=green>The first four matches are where these terms are used in the definition of Reaganomics:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c66b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_patterns = [nlp(text) for text in phrase_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6514e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(key='EconMatcher', docs=phrase_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e749019",
   "metadata": {},
   "source": [
    "## Viewing Matches\n",
    "There are a few ways to fetch the text surrounding a match. The simplest is to grab a slice of tokens from the doc that is wider than the match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03eb12a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680293220734633682 EconMatcher 41 45 by U.S. President Ronald Reagan during the 1980s. These policies are commonly associated with supply-side economics, referred to as trickle-down economics or voodoo economics by political opponents,\n",
      "3680293220734633682 EconMatcher 49 53 . These policies are commonly associated with supply-side economics, referred to as trickle-down economics or voodoo economics by political opponents, and free-market economics by political advocates\n",
      "3680293220734633682 EconMatcher 54 56 associated with supply-side economics, referred to as trickle-down economics or voodoo economics by political opponents, and free-market economics by political advocates.\n",
      "\n",
      "The\n",
      "3680293220734633682 EconMatcher 61 65 referred to as trickle-down economics or voodoo economics by political opponents, and free-market economics by political advocates.\n",
      "\n",
      "The four pillars of Reagan's economic policy were to\n",
      "3680293220734633682 EconMatcher 673 677 's New Deal policies. At the same time he attracted a following from the supply-side economics movement, which formed in opposition to Keynesian demand-stimulus economics. This movement\n",
      "3680293220734633682 EconMatcher 2986 2990 and similar organizations) lawsuits against institutions.[66] His policies became widely known as \"trickle-down economics\", due to the significant cuts in the upper tax brackets, as that\n"
     ]
    }
   ],
   "source": [
    "found_matches = matcher(doc3)\n",
    "\n",
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = doc3[start-15:end+15]                    # get the matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
