# Tokenization

- is the process of breaking up the original text into component pieces (tokens).
Tokens are the basic building blocks of a Doc object - everything that helps us understand the meaning of the text is derived from tokens and their relationship to one another.

- Prefix:
Character(s) at the beginning
- Suffix:
Character(s) at the end
- Infix:
Character(s) in between
- Exception:
split a string into several tokens or prevent a token from being split when punctuation rules are applied

