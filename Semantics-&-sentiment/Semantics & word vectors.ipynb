{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14bb7f8e",
   "metadata": {},
   "source": [
    "# Semantics and Word Vectors\n",
    "Sometimes called \"opinion mining\", [Wikipedia](https://en.wikipedia.org/wiki/Sentiment_analysis) defines ***sentiment analysis*** as\n",
    "<div class=\"alert alert-info\" style=\"margin: 20px\">\"the use of natural language processing ... to systematically identify, extract, quantify, and study affective states and subjective information.<br>\n",
    "Generally speaking, sentiment analysis aims to determine the attitude of a speaker, writer, or other subject with respect to some topic or the overall contextual polarity or emotional reaction to a document, interaction, or event.\"</div>\n",
    "\n",
    "Up to now we've used the occurrence of specific words and word patterns to perform test classifications. In this section we'll take machine learning even further, and try to extract intended meanings from complex phrases. Some simple examples include:\n",
    "* Python is relatively easy to learn.\n",
    "* That was the worst movie I've ever seen.\n",
    "\n",
    "However, things get harder with phrases like:\n",
    "* I do not dislike green eggs and ham. (requires negation handling)\n",
    "\n",
    "The way this is done is through complex machine learning algorithms like [word2vec](https://en.wikipedia.org/wiki/Word2vec). The idea is to create numerical arrays, or *word embeddings* for every word in a large corpus. Each word is assigned its own vector in such a way that words that frequently appear together in the same context are given vectors that are close together. The result is a model that may not know that a \"lion\" is an animal, but does know that \"lion\" is closer in context to \"cat\" than \"dandelion\".\n",
    "\n",
    "It is important to note that *building* useful models takes a long time - hours or days to train a large corpus - and that for our purposes it is best to import an existing model rather than take the time to train our own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cceb094",
   "metadata": {},
   "source": [
    "___\n",
    "# Word Vectors\n",
    "Word vectors - also called *word embeddings* - are mathematical descriptions of individual words such that words that appear frequently together in the language will have similar values. In this way we can mathematically derive *context*. As mentioned above, the word vector for \"lion\" will be closer in value to \"cat\" than to \"dandelion\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "859a3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy and load librairies\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48fa9eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.2746  ,   0.46242 ,  -1.1829  ,  -5.2661  ,  -2.7128  ,\n",
       "         1.8521  ,  -0.94273 ,   2.1865  ,   6.503   ,   0.6704  ,\n",
       "         1.5361  ,   2.5992  ,  -0.36233 ,   4.3965  ,  -6.5644  ,\n",
       "         1.6141  ,  -1.2897  ,   2.1184  ,  -0.63654 ,  -3.4572  ,\n",
       "        -4.3771  ,   4.2074  ,  -3.6411  ,  -0.97214 ,   1.3253  ,\n",
       "        -2.3125  ,  -3.6531  ,  -2.8398  ,   2.7913  ,  -1.53    ,\n",
       "        -2.9984  ,  -2.6357  ,   0.50615 ,  -2.6925  ,   4.3401  ,\n",
       "        -5.6017  ,   0.045691,   4.3832  ,  -0.19535 ,  -1.0751  ,\n",
       "         0.32172 ,   2.4395  ,   4.6638  ,   3.4471  ,  -3.3847  ,\n",
       "        -1.8238  ,   0.70212 ,   0.58557 ,   5.0032  ,  -3.1072  ,\n",
       "         1.2364  ,   7.4595  ,   0.057368,   1.0111  ,  -1.0827  ,\n",
       "         0.69113 ,   2.8009  ,  -3.4383  ,  -1.0599  ,  -2.2627  ,\n",
       "        -5.149   ,  -5.0636  ,   3.1405  ,   1.0793  ,  -0.72892 ,\n",
       "        -3.9939  ,  -0.69551 ,  -0.55767 ,   3.2555  ,  -2.9449  ,\n",
       "         4.7114  ,   1.6388  ,   1.3828  ,   1.4255  ,  -3.2334  ,\n",
       "        -2.274   ,  -1.8136  ,   2.2966  ,   2.5462  ,   1.0722  ,\n",
       "        -0.73447 ,   1.2148  ,  -0.9196  ,  -0.065012,   2.088   ,\n",
       "         0.57002 ,   3.5746  ,   1.7192  ,  -8.335   ,   0.71079 ,\n",
       "         0.91314 ,  -5.0107  ,   1.899   ,  -4.4658  ,   4.7993  ,\n",
       "        -0.39899 ,  -2.673   ,  -2.9354  ,   4.304   ,   1.4336  ,\n",
       "         3.7121  ,   0.34882 ,   4.6512  ,  -4.5731  ,  -4.5665  ,\n",
       "         1.5988  ,  -0.50383 ,   0.95857 ,   0.68728 ,  -0.39976 ,\n",
       "        -3.1922  ,   4.4363  ,  -0.69479 ,  -1.9528  ,   4.9376  ,\n",
       "         2.7259  ,   2.2485  ,   5.5734  ,   2.5842  ,   4.7836  ,\n",
       "        -1.0274  ,   2.2703  ,  -2.0696  ,  -1.0642  ,  -4.932   ,\n",
       "        -2.274   ,   4.1409  ,   0.73313 ,   2.1889  ,  -0.098888,\n",
       "         1.6472  ,  -2.3985  ,   2.5911  ,   3.6026  ,   1.885   ,\n",
       "         5.7822  ,  -1.4481  ,   1.8914  , -10.044   ,  -5.7452  ,\n",
       "        -4.3224  ,  -3.854   ,   2.3084  ,  -0.84018 ,  -0.40526 ,\n",
       "         4.7741  ,  -2.3271  ,   7.064   ,   0.95753 ,  -2.356   ,\n",
       "         0.83953 ,   0.40004 ,   0.33743 ,   0.8376  ,   3.9285  ,\n",
       "         0.05955 ,   2.4422  ,   4.3492  ,   3.9861  ,   2.1043  ,\n",
       "        -1.0197  ,  -0.61752 ,  -0.42999 ,  -0.1014  ,  -5.9571  ,\n",
       "        -0.53818 ,  -1.7797  ,   1.7446  ,   2.3934  ,  -0.50263 ,\n",
       "        -1.6222  ,  -0.37372 ,  -6.8938  ,   0.55018 ,  -2.267   ,\n",
       "         0.64912 ,   3.1525  ,  -2.2541  ,  -4.0384  ,   3.206   ,\n",
       "         0.14962 ,  -2.6662  ,   0.18167 ,   5.0028  ,   2.1521  ,\n",
       "         0.92419 ,   5.4163  ,  -2.2408  ,   1.6585  ,  -5.1625  ,\n",
       "         5.029   ,   0.1026  ,  -0.44542 ,   2.0557  ,   3.7778  ,\n",
       "         3.8679  ,  -2.7135  ,   5.3242  ,  -3.2916  ,   5.6421  ,\n",
       "         5.0466  ,   1.6072  ,  -1.3206  ,   4.2044  ,  -0.33793 ,\n",
       "        -3.1139  ,   2.8841  ,  -3.1565  ,  -2.9832  ,  -0.23235 ,\n",
       "         2.3259  ,   3.5477  ,  -2.1299  ,  -1.8344  ,   2.7271  ,\n",
       "         1.5568  ,   5.6865  ,   0.9412  ,  -2.6412  ,  -5.3254  ,\n",
       "         1.3494  ,  -0.47159 ,   2.4979  ,  -1.5568  ,  -1.6911  ,\n",
       "        -2.1842  ,   6.0319  ,   0.022573,   2.3824  ,  -1.1002  ,\n",
       "         0.90216 ,  -1.9113  ,   1.5527  ,   5.7413  ,  -3.1956  ,\n",
       "         0.68655 ,  -1.6068  ,   1.7404  ,  -3.2142  ,   6.4783  ,\n",
       "         1.7548  ,  -2.9795  ,   0.97631 ,  -0.018354,  -0.6379  ,\n",
       "         0.80559 ,   3.1923  ,   3.3335  ,   4.3068  ,  -1.0819  ,\n",
       "        -1.3839  ,  -4.7626  ,  -4.6637  ,  -1.2201  ,  -3.2741  ,\n",
       "         1.5204  ,   0.78119 ,   8.7339  ,   1.6009  ,  -0.79332 ,\n",
       "         5.8416  ,  -1.485   ,   1.5978  ,   2.9746  ,  -0.30759 ,\n",
       "        -1.8023  ,  -4.8344  ,   1.2817  ,  -2.5469  ,   2.6517  ,\n",
       "         1.4881  ,   2.1952  ,  -0.12652 ,   1.2223  ,   0.44763 ,\n",
       "        -3.1445  ,  -2.2051  ,  -4.1785  ,  -3.6539  ,   5.1929  ,\n",
       "         0.78457 ,  -1.2312  ,   5.5624  ,  -1.8462  ,   6.1262  ,\n",
       "        -1.6653  ,  -2.7557  ,  -0.066465,  -3.6362  ,   5.2005  ,\n",
       "        -1.2865  ,   2.8855  ,   6.1219  ,   1.7824  ,   1.4264  ,\n",
       "        10.628   ,  -0.36028 ,   1.9268  ,  -7.835   ,   0.57865 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u\"lion\").vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac31f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u\"The quick brown fox jumped\").vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da68bfe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u\"fox\").vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438875ed",
   "metadata": {},
   "source": [
    "## Identifying similar vectors\n",
    "The best way to expose vector relationships is through the `.similarity()` method of Doc tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d93d557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u\"lion cat pet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ff2aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion lion 1.0\n",
      "lion cat 0.3854507803916931\n",
      "lion pet 0.20031583309173584\n",
      "cat lion 0.3854507803916931\n",
      "cat cat 1.0\n",
      "cat pet 0.732966423034668\n",
      "pet lion 0.20031583309173584\n",
      "pet cat 0.732966423034668\n",
      "pet pet 1.0\n"
     ]
    }
   ],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a607bfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<table><tr><th></th><th>lion</th><th>cat</th><th>pet</th></tr><tr><td>**lion**</td><td>1.0</td><td>0.3855</td><td>0.2003</td></tr><tr><td>**cat**</td><td>0.3855</td><td>1.0</td><td>0.733</td></tr><tr><td>**pet**</td><td>0.2003</td><td>0.733</td><td>1.0</td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For brevity, assign each token a name\n",
    "a,b,c = tokens\n",
    "\n",
    "# Display as a Markdown table (this only works in Jupyter!)\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f'<table><tr><th></th><th>{a.text}</th><th>{b.text}</th><th>{c.text}</th></tr>\\\n",
    "<tr><td>**{a.text}**</td><td>{a.similarity(a):{.4}}</td><td>{b.similarity(a):{.4}}</td><td>{c.similarity(a):{.4}}</td></tr>\\\n",
    "<tr><td>**{b.text}**</td><td>{a.similarity(b):{.4}}</td><td>{b.similarity(b):{.4}}</td><td>{c.similarity(b):{.4}}</td></tr>\\\n",
    "<tr><td>**{c.text}**</td><td>{a.similarity(c):{.4}}</td><td>{b.similarity(c):{.4}}</td><td>{c.similarity(c):{.4}}</td></tr>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516c3c8",
   "metadata": {},
   "source": [
    "As expected, we see the strongest similarity between \"cat\" and \"pet\", the weakest between \"lion\" and \"pet\", and some similarity between \"lion\" and \"cat\". A word will have a perfect (1.0) similarity with itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed6789",
   "metadata": {},
   "source": [
    "### Opposites are not necessarily different\n",
    "Words that have opposite meaning, but that often appear in the same *context* may have similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7a87362",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u\"like love hate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3daa580d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<table><tr><th></th><th>like</th><th>love</th><th>hate</th></tr><tr><td>**like**</td><td>1.0</td><td>0.5213</td><td>0.5065</td></tr><tr><td>**love**</td><td>0.5213</td><td>1.0</td><td>0.5708</td></tr><tr><td>**hate**</td><td>0.5065</td><td>0.5708</td><td>1.0</td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a,b,c = tokens\n",
    "\n",
    "# Display as a Markdown table (this only works in Jupyter!)\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f'<table><tr><th></th><th>{a.text}</th><th>{b.text}</th><th>{c.text}</th></tr>\\\n",
    "<tr><td>**{a.text}**</td><td>{a.similarity(a):{.4}}</td><td>{b.similarity(a):{.4}}</td><td>{c.similarity(a):{.4}}</td></tr>\\\n",
    "<tr><td>**{b.text}**</td><td>{a.similarity(b):{.4}}</td><td>{b.similarity(b):{.4}}</td><td>{c.similarity(b):{.4}}</td></tr>\\\n",
    "<tr><td>**{c.text}**</td><td>{a.similarity(c):{.4}}</td><td>{b.similarity(c):{.4}}</td><td>{c.similarity(c):{.4}}</td></tr>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7ac54bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.vectors.shape # unique words in the vacabulary that we have vectors for it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d3e55a",
   "metadata": {},
   "source": [
    "## Vector norms\n",
    "It's sometimes helpful to aggregate 300 dimensions into a [Euclidian (L2) norm](https://en.wikipedia.org/wiki/Norm_%28mathematics%29#Euclidean_norm), computed as the square root of the sum-of-squared-vectors. This is accessible as the `.vector_norm` token attribute. Other helpful attributes include `.has_vector` and `.is_oov` or *out of vocabulary*.\n",
    "\n",
    "For example, our 685k vector library may not have the word \"[nargle](https://en.wikibooks.org/wiki/Muggles%27_Guide_to_Harry_Potter/Magic/Nargle)\". To test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dda6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u\"dog cat nargle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "754f84e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 75.254234 False\n",
      "cat True 63.188496 False\n",
      "nargle False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm,token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a2ee4",
   "metadata": {},
   "source": [
    "Indeed we see that \"nargle\" does not have a vector, so the vector_norm value is zero, and it identifies as *out of vocabulary*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f8733",
   "metadata": {},
   "source": [
    "## Vector arithmetic\n",
    "Believe it or not, we can actually calculate new vectors by adding & subtracting related vectors. A famous example suggests\n",
    "<pre>\"king\" - \"man\" + \"woman\" = \"queen\"</pre>\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9edb4436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02f4298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ad8dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# king - man + woman ---> NEW_VECTOR similar Queen, princess, highness\n",
    "new_vector = king - man + woman\n",
    "computed_similarities = []\n",
    "\n",
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors and mixed-case words:\n",
    "    if word.has_vector:\n",
    "        if word.is_lower:\n",
    "            if word.is_alpha:\n",
    "                similarity = cosine_similarity(new_vector, word.vector)\n",
    "                computed_similarities.append((word, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b491ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_similarities = sorted(computed_similarities, key=lambda item:-item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c5bee81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'and', 'that', 'havin', 'where', 'she', 'they', 'woman', 'somethin', 'there']\n"
     ]
    }
   ],
   "source": [
    "print([t[0].text for t in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8927d128",
   "metadata": {},
   "source": [
    "So in this case, \"king\" was still closer than \"queen\" to our calculated vector, although \"woman\" did show up!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
